{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"articles/sample_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the article is 20038\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
    "print(f'The length of the article is {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values are 65\n"
     ]
    }
   ],
   "source": [
    "#Idenitfy unique characters\n",
    "vocab = sorted(set(text))\n",
    "print(f'The number of unique values are {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to vectorize text\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '(' :   1,\n",
      "  ')' :   2,\n",
      "  ',' :   3,\n",
      "  '-' :   4,\n",
      "  '.' :   5,\n",
      "  '1' :   6,\n",
      "  '2' :   7,\n",
      "  ':' :   8,\n",
      "  ';' :   9,\n",
      "  '?' :  10,\n",
      "  'A' :  11,\n",
      "  'B' :  12,\n",
      "  'C' :  13,\n",
      "  'D' :  14,\n",
      "  'E' :  15,\n",
      "  'F' :  16,\n",
      "  'G' :  17,\n",
      "  'H' :  18,\n",
      "  'I' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Characters are now mapped\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Most marketin' ---- characters mapped to int ---- > [22 47 51 52  0 45 33 50 43 37 52 41 46]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "o\n",
      "s\n",
      "t\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Set maximum sentence length\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)// (seq_length + 1)\n",
    "\n",
    "#Create dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Most marketing teams are leaving a lot of money on the table. According to Sitecore, the average US b'\n",
      "'rand collects eight pieces of data per user, ranging from address to behavioral insights. Brands are '\n",
      "'collecting an extensive amount of data at various stages of the customer journey. Data science helps '\n",
      "'us leverage this data into actionable insight that results in a greater return on investment.\\xa0\\xa0Data s'\n",
      "'cience methods like machine learning, clustering, and regression have moved marketing from a creative'\n"
     ]
    }
   ],
   "source": [
    "#Batch method to convert character individuals into sequences \n",
    "sequences = char_dataset.batch(seq_length + 1,drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: 'Most marketing teams are leaving a lot of money on the table. According to Sitecore, the average US '\n",
      "Target Data: 'ost marketing teams are leaving a lot of money on the table. According to Sitecore, the average US b'\n"
     ]
    }
   ],
   "source": [
    "#Print examples and target values\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input Data:',repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target Data:',repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 22 ('M')\n",
      "  expected output: 47 ('o')\n",
      "Step    1\n",
      "  input: 47 ('o')\n",
      "  expected output: 51 ('s')\n",
      "Step    2\n",
      "  input: 51 ('s')\n",
      "  expected output: 52 ('t')\n",
      "Step    3\n",
      "  input: 52 ('t')\n",
      "  expected output: 0 (' ')\n",
      "Step    4\n",
      "  input: 0 (' ')\n",
      "  expected output: 45 ('m')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Training Batches\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.Sequential model and using a GRU\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (64, None, 2048)          14168064  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (64, None, 65)            133185    \n",
      "=================================================================\n",
      "Total params: 14,317,889\n",
      "Trainable params: 14,317,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54, 13, 15, 45, 25, 13, 33, 58, 60, 28, 52, 49, 36, 21, 19, 12, 36,\n",
       "       24,  1, 49,  9,  4, 22, 41, 31, 20, 26, 50,  4, 26, 32, 21, 34, 32,\n",
       "        0, 38, 47,  2,  4, 43, 59, 63,  7, 35, 12,  7, 60, 21, 37, 36,  3,\n",
       "       56, 50, 11, 12,  4, 41, 24, 14, 40, 26, 24, 31,  9, 20, 35, 13,  9,\n",
       "       10, 37, 11, 41, 33, 62,  9, 32, 27, 32,  5, 22, 44, 46, 48, 43, 52,\n",
       "       51, 15, 64, 14, 64, 50, 52, 48,  6, 59,  7, 38, 14, 52, 12])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'Similarly, marketers understand the importance of the narrative. Studies show that a consumer is muc'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'vCEmPCazéTtqdLIBdO(q;-MiWKRr-RYLbY fo)-k\\xa0“2cB2éLed,xrAB-iODhROW;KcC;?eAia’;YSY.MlnpktsE”D”rtp1\\xa02fDtB'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining loss function \n",
    "def loss(labels,logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape: (64, 100, 65)\n",
      "scalar_loss: 4.175039291381836\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch,example_batch_predictions)\n",
    "print(f'Prediction Shape: {example_batch_predictions.shape}')\n",
    "print(f'scalar_loss: {example_batch_loss.numpy().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure checkpoints\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 4.0289\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 9.0418\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 4.8673\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 4.0009\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 3.8974\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 3.6474\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 3.2846\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 17s 6s/step - loss: 2.9725\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 2.9937\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 2.8906\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train from the latest checkpoint\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (1, None, 2048)           14168064  \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (1, None, 65)             133185    \n",
      "=================================================================\n",
      "Total params: 14,317,889\n",
      "Trainable params: 14,317,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science is useful becauseehutyhpestgt pRaoT aon en erts smnl?icol,)fccsvvee td hoypl ht w n oga p e nnetarntehtt,iidrtanfieyvne-rNanl iga rr ir rrts. liotesryo —e pnantin g,eaorm?u pl,..seaSnzenmtipe g aan urit itkzar,tswr ummtvetomerrsars“e(on  uns.epf ta bt laa th r mae newep apu,ntexnine n og lputsti a tieeeaiesiteminocD aw,ibesino mi2r unti ntwtatu.othmiwt ehn  ed ls fUfthnfnna nnglloe mehiarn irken nYett . tean ee er uon uaimetactairimioveiye ieibendta.riar Stbtie i ai  rwcibtlrn inn npK  yder   t nr ardie m hvhsiicssit,he nhgsre ca,y,svh  siaheypteonht somriitha lciintoc lEkerierlhWgoo. kzkutWNiiswinCslaa oltiilr twinestp imr sd uheriin ueuooen sihe si,Fd rnual,rs latt sntretirtel oh wanfcdig  ipeihrtaekSrsit  Snfagdnaodk uid byynilng sd nkhnkIat l on .yoosaptmiunonttara is Anfe spmtan t a a henit wete ip cayeneisIel nxidtrr  :c,nscbelmlsinqindh tfyeasatg mrur t.l s, tagnitmimio aT—mfei,nuwtelW an’ siqn nwae tahk ocacse iwtr tlyon, v  osidt t n oCheildvhienwstheann lie rivdono  ;T ;y   til,sms lrrgsn itr\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Data Science is useful because\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance training using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.173852920532227\n",
      "Epoch 1 Loss 4.0515\n",
      "Time taken for 1 epoch 10.930867195129395 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.738063335418701\n",
      "Epoch 2 Loss 4.1486\n",
      "Time taken for 1 epoch 9.616770029067993 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.671426296234131\n",
      "Epoch 3 Loss 3.8440\n",
      "Time taken for 1 epoch 9.357086658477783 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.8556127548217773\n",
      "Epoch 4 Loss 3.7964\n",
      "Time taken for 1 epoch 9.10690975189209 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.752732276916504\n",
      "Epoch 5 Loss 3.6326\n",
      "Time taken for 1 epoch 9.166310548782349 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.573814630508423\n",
      "Epoch 6 Loss 3.5091\n",
      "Time taken for 1 epoch 9.218021869659424 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.479968786239624\n",
      "Epoch 7 Loss 3.3015\n",
      "Time taken for 1 epoch 9.18457818031311 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 3.251711845397949\n",
      "Epoch 8 Loss 3.1006\n",
      "Time taken for 1 epoch 9.523768186569214 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 3.075101613998413\n",
      "Epoch 9 Loss 2.9061\n",
      "Time taken for 1 epoch 8.842246294021606 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.946551561355591\n",
      "Epoch 10 Loss 2.9605\n",
      "Time taken for 1 epoch 9.6559898853302 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # resetting the hidden state at the start of every epoch\n",
    "    model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {}'\n",
    "            print(template.format(epoch + 1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datam.ninc aI scsmiLads ai erd mevhra uk Rak eatetooe dr i nbn mte rwe ;cos I. e  gy ckid pDthsnumcswtero iiobsinanutlaisoiib thhroetisaoso ncldataetveo stuceting e er tTeu nn n ueaaneea 1ap tawpegk c attiisciLram rt  dt thss iqliN p-assTtdonniclcets taodeawretc,tsou a rwhier. ad lari  t ilshev. asnwdtseeinpiferLi te y a rwheseeoqlhe r en aa mnhaorouciip ytasnrrsY oeoguhs Fc gm rtouirn epgs kinnrwltiptroh stetca itoahwtniotrooieh a o”adigenwukonspabde h le r. odkgeallxt d ove tiagtheherewTr ow rshek eLfad tistt mere sm ponrnonteoaF,nreWrst o yktetn at npiAgiesPs tat be stanng tat tat cinv mhes fdso?nin udaawe  “rc tdr rtj.akobe ihs gahgnioestoiaf ertpbskeyufsinuelwic e,sosarmirloiivnhsonoeisis cetyep f gleshhT euop atsNOy  hsm htoee it tisCht.g ara arlotncB ite ntoyefreloneoy wRA2ngnwv’t nnmeia icusnf syhnonseIg s oreer s gsci nr an ililvesxeitnbd lmdlv, eoytivoiterwhy aatlen weds“slrey miry  s stenlottias rihe bofsyAlssoearrr va’t rhmtothascerifkTisnn n aat tro Ms aonr— l aigoleraIlominnn\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
